\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{amsmath}
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage[english]{babel}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{subcaption}
%\usepackage{subfigure}
\usepackage{pdfpages}
%\usepackage{indentfirst}
\setlength{\parindent}{2em}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\newlist{steps}{enumerate}{1}
\setlist[steps, 1]{label = Step \arabic*:}
%\usepackage{natbib}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\usepackage{listings}
%\usepackage{minted}
%\def\thesection{\Roman{section}}
%\definecolor{bg}{rgb}{0.95,0.95,0.95}
\begin{document}
\noindent
\large
\textbf{State Estimation for Robotics Solution} \hfill  \\
\large
\\
\normalsize 
Thomas\hfill \\
  My personal solution of exercises in "State Estimation for Robotics", D. Barfoot. \hfill  \\
Post on \url{ https://github.com/taohu1994/State_Estimation_for_Robotics_solution}
\\
May 2021


\section{Chapter 2. Primer on Probability Theory}
\subsection{2.5.1}
Let $\mathbf{u}= [u_1,\dots,u_n]^T$, $\mathbf{v}= [v_1,\dots,v_n]^T$, we have
\begin{equation*}
    \mathbf{u}^T\mathbf{v} = u_1v_1+u_2v_2+\cdot+u_nv_nï¼Œ
\end{equation*}
and
\begin{equation*}
     \mathbf{v}^T\mathbf{u} =\begin{bmatrix}
     v_1u_1 & \dots & \dots & v_1u_n  \\
     \vdots & v_2u_2 &  &  \vdots\\
    \vdots   &   & \ddots  & \vdots \\
     v_nu_1   & \dots & \dots   & v_nu_n
     \end{bmatrix}.
\end{equation*}
It is clear that $  \mathbf{u}^T\mathbf{v} = \text{tr}( \mathbf{v}^T\mathbf{u})$.
\subsection{2.5.2}
Recapping the equation (2.29),
\begin{equation*}
    I(\mathbf{x},\mathbf{y}) = H(\mathbf{x})+H(\mathbf{y})-H(\mathbf{x},\mathbf{y}).
\end{equation*}
For two independent variables $\mathbf{x},\mathbf{y}$, we have 
\begin{equation*}
    I(\mathbf{x},\mathbf{y}) = 0.
\end{equation*}
Then, it is clear that
\begin{equation*}
      H(\mathbf{x})+H(\mathbf{y})=H(\mathbf{x},\mathbf{y}).
\end{equation*}


\subsection{2.5.3}
For a Gaussian random variable, $\mathbf{x}\sim \mathcal{N}(\boldsymbol{\boldsymbol{\mu},\boldsymbol{\Sigma}})$, Its covariance matrix could be presented
\begin{equation*}
\begin{split}
      \boldsymbol{\Sigma} &= E\{(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^T\}\\
      & = E\{\mathbf{x}\mathbf{x}^T\}-2E\{(\mathbf{x}\boldsymbol{\mu}^T\}+\boldsymbol{\mu}\boldsymbol{\mu}^T \\
      & =  E\{\mathbf{x}\mathbf{x}^T\} - \boldsymbol{\mu}\boldsymbol{\mu}^T .
\end{split}
\end{equation*}
We rewrite it as
\begin{equation*}
     E\{\mathbf{x}\mathbf{x}^T\} =    \boldsymbol{\Sigma}+\boldsymbol{\mu}\boldsymbol{\mu}^T.
\end{equation*}

\subsection{2.5.4}

\subsection{2.5.5}

\subsection{2.5.6}
Product of $K$ Gaussian PDFs could be written
\begin{equation*}
    \begin{split}
      & \eta \prod_{k=1}^K\exp\Big(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_k)\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_k)^T\Big)\\
       &=\eta \exp\big( -\frac{1}{2}\sum_{k=1}^K(\mathbf{x}-\boldsymbol{\mu}_k)\boldsymbol{\Sigma}_k^{-1}(\mathbf{x}-\boldsymbol{\mu}_k)^T \big)\\
       &=\eta \exp\Big( -\frac{1}{2}\big(\mathbf{x}^T\sum_{i=1}^K\boldsymbol{\Sigma}_k^{-1}\mathbf{x}-\mathbf{x}^T\sum_{k=1}^K \boldsymbol{\Sigma}_k^{-1}\boldsymbol{\mu}_k-(\sum_{k=1}^K \boldsymbol{\mu}_k^T\boldsymbol{\Sigma}_k^{-1})\mathbf{x}+\sum_{k=1}^K \boldsymbol{\mu}_k^T\boldsymbol{\Sigma}_k^{-1}\boldsymbol{\mu}_k\big)\Big)\\
        &=\eta \exp\Big( -\frac{1}{2}\big(\mathbf{x}^T\boldsymbol{\Sigma}^{-1}\mathbf{x}-\mathbf{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}-\boldsymbol{\mu}^T\boldsymbol{\Sigma}^{-1}\mathbf{x}+\sum_{k=1}^K \boldsymbol{\mu}_k^T\boldsymbol{\Sigma}_k^{-1}\boldsymbol{\mu}_k\big)\Big),
    \end{split}
\end{equation*}
where $\boldsymbol{\Sigma}^{-1} = \sum_{k=1}^K \boldsymbol{\Sigma}_k^{-1}$ and $\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu} =  \sum_{k=1}^K \boldsymbol{\Sigma}_k^{-1}\boldsymbol{\mu}_k$.
Therefor for $\boldsymbol{\mu}$, we have
\begin{equation*}
   \boldsymbol{\mu} =  \boldsymbol{\Sigma} \sum_{k=1}^K \boldsymbol{\Sigma}_k^{-1}\boldsymbol{\mu}_k,
\end{equation*}
where $ \boldsymbol{\Sigma}\boldsymbol{\Sigma}^{-1} = \mathbf{I}$. Then we have
\begin{equation*}
     \boldsymbol{\mu}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu} =  \sum_{k=1}^K(\boldsymbol{\mu}_k^T \boldsymbol{\Sigma}_k^{-1})\boldsymbol{\Sigma}
\end{equation*}
\subsection{2.5.7}
Let $\boldsymbol{\mu}_i, \boldsymbol{\Sigma}_i$ denote the mean and variance of variable $\mathbf{x}_i$. As $\forall i,j\in[1,K], i\neq j, \mathbf{x}_i,\mathbf{x}_j$ are statistically independent. We have $E[(\mathbf{x}_i-\boldsymbol{\mu}_i)(\mathbf{x}_i-\boldsymbol{\mu}_i)^T] =\boldsymbol{\Sigma}_i $ and $E[(\mathbf{x}_i-\boldsymbol{\mu}_i)(\mathbf{x}_j-\boldsymbol{\mu}_j)^T] =\mathbf{0}$.
The mean of $\mathbf{x}$ rewrite 
\begin{equation*}
    E[\mathbf{x}] = \sum_{(i=1)}^K \omega_i E[\mathbf{x}_i] = \sum_{(i=1)}^K \omega_i\boldsymbol{\mu}_i = \boldsymbol{\mu}
\end{equation*}
The variance of $\mathbf{x}$ could be presented
\begin{equation*}
\begin{split}
     Var(\mathbf{x}) &  = E[\big(\omega_1(\mathbf{x}_1-\boldsymbol{\mu}_1)+\dots+\omega_K(\mathbf{x}_K-\boldsymbol{\mu}_K)\big)\big(\omega_1(\mathbf{x}_1-\boldsymbol{\mu}_1)+\dots+\omega_K(\mathbf{x}_K-\boldsymbol{\mu}_K)\big)^T]\\
     & = \sum_{i=1}^K\omega_i^2E\{(\mathbf{x}_i-\boldsymbol{\mu}_i)(\mathbf{x}_i-\boldsymbol{\mu}_i)^T\}+\sum_{\forall i,j\in [1,K],i\neq j}\omega_i\omega_jE\{(\mathbf{x}_i-\boldsymbol{\mu}_i)(\mathbf{x}_j-\boldsymbol{\mu}_j)^T\} \\
     & = \sum_{i=1}^K\omega_i^2E\{(\mathbf{x}_i-\boldsymbol{\mu}_i)(\mathbf{x}_i-\boldsymbol{\mu}_i)^T\} \\
     & = \sum_{i=1}^K\omega_i^2\boldsymbol{\Sigma}_i.
\end{split}   
\end{equation*}



\subsection{2.5.8}
Note $\mathbf{x}=[x_1,x_2,\dots,x_K]^T$. For scalar $x_i\in\mathbf{x}$, we have $x_i\sim \mathcal{N}(0,1)$. And $\forall i,j\in[1,K],i\neq j,E\{x_ix_i\}=1,E\{x_ix_j\}=0 $. We write the mean of $\mathbf{y}$ as
\begin{equation*}
\begin{split}
     E\{\mathbf{x}^T\mathbf{x}\} &= E\{x_1x_1+\cdots+ x_Kx_K\}\\
     & =  E\{x_1x_1\}+  \cdots + E\{x_Kx_K\} \\
     & = \sum_{i=1}^K E\{x_ix_i\} \\
     & = K.
\end{split}
\end{equation*}
The variance of  $\mathbf{y}$,

\begin{equation*}
\begin{split}
\text{ Var}(\mathbf{y}) &= E\{(\mathbf{x}^T\mathbf{x}-K)(\mathbf{x}^T\mathbf{x}-K)^T\} \\
& = E\{\mathbf{x}^T\mathbf{x}\mathbf{x}^T\mathbf{x}\}-2E\{\mathbf{x}\mathbf{x}^T\}K+ K^2\\
& =  E\{(x_1x_1+\cdots+x_Kx_K)(x_1x_1+\cdots+x_Kx_K)\} -K^2 \\
& = E\{\sum_{i=1}^K x_ix_ix_ix_i\}+ E\{\sum_{\forall i,j\in[1,K], i\neq j} x_ix_ix_jx_j\}-K^2\\
& = K E\{x_ix_ix_ix_i\} + (K^2-K) E\{x_ix_ix_jx_j\} - K^2 \\
& = 2K.
\end{split}
\end{equation*}


Note that here we use the Isserlis' Theorem and equation (2.40), that  
\begin{equation*}
    E\{x_ix_ix_ix_i\} = 3E\{x_ix_i\}E\{x_ix_i\} = 3.
\end{equation*} and 
\begin{equation*}
    E\{x_ix_ix_jx_j\} =E\{x_ix_i\}E\{x_jx_j\}+2E\{x_ix_j\}E\{x_ix_j\}=1.
\end{equation*}




\newpage
\bibliographystyle{ieeetr}
\bibliography{ref}
\end{document}
